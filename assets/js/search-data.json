{
  
    
        "post0": {
            "title": "Title",
            "content": "Hi everyone, this is part 2 of the series &quot;Window Store Explonatory Data Analysis (EDA)&quot;. If you haven&#39;t checked out part 2 already then please use this link to access the article: . https://www.kaggle.com/sanchitagarwal/windows-store-eda-1 . A quick look at the data revealed that the majority of the data (97%) are free apps (price = 0), so I decided to stratify the dataset into categories &quot;Free&quot; and &quot;Paid&quot; based on the value of the PRICE column; The intutition being that the behviour of the population intenting to purchase an application and the resultant characterstics will be significantly different than those who aren&#39;t. Nevertheless, later on, I shall seek correlation between both the stratas. . Lets start with the &quot;Paid&quot; category first. . P.S: SQL related code have been commented and replaced with Kaggle appropriate code. . import pandas as pd import matplotlib.pyplot as plt from matplotlib import dates %matplotlib inline import seaborn as sns . &quot;&quot;&quot;server = &quot;You seriously thought&quot; user = &quot;that I&#39;m stupid enough to&quot; password = &quot;expose my credentials to the public ? &quot; connection = pymssql.connect(server, user, password, &quot;master&quot;) with connection: with connection.cursor(as_dict = True) as cursor: sql = &quot;SELECT * FROM windows_store WHERE price != 0.0&quot; cursor.execute(sql) result = cursor.fetchall() #print(result) paid_apps_df = pd.DataFrame(result)&quot;&quot;&quot; apps_df = pd.read_csv(&quot;../input/windows-store/msft.csv&quot;) apps_df[&quot;Price&quot;].fillna(value = &quot;Free&quot;, inplace = True) paid_apps_df = apps_df.loc[apps_df.Price != &quot;Free&quot;] paid_apps_df[&#39;Date&#39;] = pd.to_datetime(paid_apps_df[&#39;Date&#39;]) paid_apps_df[&#39;Price&#39;] = paid_apps_df[&#39;Price&#39;].replace(&quot;[ ₹, ,]&quot;,&quot;&quot;,regex=True).astype(float) paid_apps_df.head() . /opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy /opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . Name Rating No of people Rated Category Date Price . 5163 Bluestacks app player: guide | 1.5 | 760 | Books | 2019-12-16 | 144.0 | . 5164 Adobe Acrobat Reader DC PC Guide | 2.0 | 163 | Books | 2020-06-28 | 144.0 | . 5165 Play Books Online (Client For Google Books) | 2.5 | 292 | Books | 2016-01-14 | 54.0 | . 5166 WinRAR PC-Guide | 1.0 | 523 | Books | 2019-11-22 | 99.0 | . 5167 Windows Movie Maker : PC Guide | 1.0 | 515 | Books | 2020-06-22 | 144.0 | . P.S: Its important to convert the DATE column into pandas datetime datatype to avoid any conflicts when doing datetime analysis through Pandas. . paid_apps_df.describe() . Rating No of people Rated Price . count 158.000000 | 158.000000 | 158.000000 | . mean 2.477848 | 546.721519 | 370.862025 | . std 1.447978 | 269.240566 | 575.466987 | . min 1.000000 | 101.000000 | 54.000000 | . 25% 1.000000 | 306.500000 | 110.250000 | . 50% 2.000000 | 533.000000 | 269.000000 | . 75% 4.000000 | 796.000000 | 269.000000 | . max 5.000000 | 993.000000 | 5449.000000 | . In the RATING column we can see that the average rating for paid apps is a mere 2.47, a horrondous result considering that one would expect a better user experience on apps which have been paid for. . In the PRICE column, the range is pretty large (5449 - 54 = 5395), and so is the standard deviation, meaning there is a lot of variance in the prices. Interesting thing to know is that both the 50th &amp; 75th percentile equals 269, meaning that 269 is the most commonly used to price apps. This can be verified by finding the mode of the PRICE column. . paid_apps_df[&quot;Price&quot;].mode() . 0 269.0 dtype: float64 . Voila!! . paid_apps_df.Category.describe() . count 158 unique 3 top Books freq 56 Name: Category, dtype: object . paid_apps_df.Category.unique() . array([&#39;Books&#39;, &#39;Business&#39;, &#39;Developer Tools&#39;], dtype=object) . &quot;Book&quot; being the top category in paid apps confirms books to be the most popular product a person is willing to pay for, though abeit by a very small margin (56/158 = 0.3544 * 100 = 35.44%). Also to note that there are 8 records with NULL entry for the CATEGORY column. . Now lets do some plotting! . df_plot_count = paid_apps_df[[&quot;Rating&quot;, &quot;Date&quot;]].groupby(pd.Grouper(key=&quot;Date&quot;, freq=&#39;Y&#39;)).count() #print(df_plot_count) plt.figure(figsize=(15,6)) plt.title(&quot;Paid Apps launched per year&quot;) plot = sns.lineplot(data=df_plot_count) plot.xaxis.set_major_formatter(dates.DateFormatter(&quot;%Y&quot;)) plot.set(ylabel = &quot;Count&quot;) plot.legend(labels=[&quot;Count&quot;]) plt.show() . This graph shows that the number of paid apps launched is increasing almost exponentially. This tells us the increase in popularity of Windows app ecosystem. . I am interested in knowing if there is any seasonality trend amongest PRICE, RATING and NO_OF_PEOPLE. . df_plot = paid_apps_df[[&quot;Date&quot;, &quot;Rating&quot;, &#39;No of people Rated&#39;, &quot;Price&quot;]] df_plot.set_index(&#39;Date&#39;, inplace=True) df_plot = df_plot.groupby(pd.Grouper(freq=&#39;M&#39;)).mean().dropna(how=&quot;all&quot;) plt.figure(figsize=(15,6)) plt.title(&quot;PRICE Seasonality analysis&quot;) plot = sns.lineplot(data= df_plot[&quot;Price&quot;]) plot.set(ylabel = &quot;Price&quot;) plot.xaxis.set_minor_locator(dates.MonthLocator()) plot.xaxis.set_major_formatter(dates.DateFormatter(&quot;%Y&quot;)) plt.show() . From 2011 to 2013, the increase was almost linear but later on there are sudden surges followed by a decrease, like a sine wave. . Also to note that for years 2013, 2015, 2017, 2018 and 2020, surge in price are detected in the first months (Jan, Feb, March). What could cause this pattern to reemerge ? Something to ponder about. . Lastly, from 2020 onwards, the prices are in the low territory compared to previous year&#39;s. I believe this can be attributed to the change in pricing strategy (where apps are free to download with additional content available to purchase) and/or increase in piracy. . plt.figure(figsize=(15,6)) plt.title(&quot;RATING seasonality analysis&quot;) plot = sns.lineplot(data= df_plot[[&quot;Rating&quot;]]) plot.xaxis.set_minor_locator(dates.MonthLocator()) plot.set(ylabel = &quot;Rating&quot;) plot.xaxis.set_major_formatter(dates.DateFormatter(&quot;%Y&quot;)) plt.show() . We can see that there is an increase in volatility as we move further down the x-axis. This could be attributed to the increase in number of apps published through the years as explored in the first graph. . plt.figure(figsize=(15,6)) plt.title(&quot;No of people seasonality analysis&quot;) plot = sns.lineplot(data= df_plot[[&quot;No of people Rated&quot;]]) plot.xaxis.set_minor_locator(dates.MonthLocator()) plot.xaxis.set_major_formatter(dates.DateFormatter(&quot;%Y&quot;)) plt.show() . This graph and the previous graph are almost similar (which makes perfect sense since the set of people who rated the apps is subset of set of people who have downloaded the app, i.e not all people who downloaded the app may have rated the app but those who have rated the app have definitely downloaded it!). . Nothing new to explore here. . Now lets find any correlation between PRICE and RATING. . plt.figure(figsize=(15,6)) plt.title(&quot;Correlation between Price &amp; Rating&quot;) plot = sns.scatterplot(y=paid_apps_df[&quot;Price&quot;], x=paid_apps_df[&quot;Rating&quot;]) plt.show() . For the 4 rating, the range of price is the largest, while the perfect 5 rating has been achieved by relatively inexpensive apps. This affirms the fact that the mere price tag of an application is not the sole indication of the user experience. This is further stated by the 3 rating achieved by the humongously priced &quot;5449&quot; app. . Thats it for now. In the next article I will continue with my analysis of the &quot;Free apps&quot; strata of the dataset. Please don&#39;t hesitate to share your feedbacks. Thanks! .",
            "url": "https://sanchit-agarwal.github.io/2021/07/20/windows-store-eda-2.html",
            "relUrl": "/2021/07/20/windows-store-eda-2.html",
            "date": " • Jul 20, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Windows Store dataset Exploatory Data Analysis",
            "content": "Today I will be doing Exploratory Data Analysis (EDA) on The Windows Store dataset by @Vishnu. . I love Docker to keep everything compartmentalize and neat. Plus, it also ensures you that you can also easily shift (and scale) your system to any of your favorite cloud provider like AWS, Azure, Google Cloud etc. If you don&#39;t know what Docker is then I highly suggest you to check out this link . https://www.zdnet.com/article/what-is-docker-and-why-is-it-so-darn-popular/ . For meeting my relational database needs, I am choosing an official docker image of SQL Server 2019 version. One free tip for you guys: . if you are doing anything with your containers which you would have to access later, don&#39;t forget to use persistant storage. And please use volumes, not bind mounts (Will explain this in some future article). . For setting up my Python IDE, I downloaded an official docker image of Jupyter notebook (not the Jupyter lab), which was super easy to setup. Just don&#39;t forget to enable port forwarding between the container and the host on 8888. I first thought of building my own python notebook docker image from scratch but then I procastinated it till the near future. Here&#39;s the link to that image: . https://hub.docker.com/r/jupyter/minimal-notebook . For those who have just entered the Docker realm, it&#39;s dangerous to go alone. Take these docker commands! . docker rename &lt;old_container_name&gt; &lt;new_container_name&gt; . Self explantory . | docker cp &lt;host_filepath&gt; &lt;container_filepath&gt; . For copying a file located in your local system to a container. . | docker ps -a . Lists all docker containers . | docker exec -it &lt;container_name&gt; &quot;bash&quot; . Opens a bash terminal connected with the forementioned container . | docker images . Lists all images downloaded on the local system. You can use docker images rm to do housecleaning.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; The dataset contains 6 columns, which are pretty self explanatory, so I won&#39;t be explaning them here. They are also mentioned in the dataset&#39;s description. . A quick look at the dataset through Excel revealed that the date column contained values in 2 different formats, which had to be fixed before I can import it into SQL Server. Even after using Excel&#39;s default data formatting,only values belonging to one particular format got converted into DateTime datatype, and the rest still remained as text strings. Fear not! As by using some clever usage of Excel formulas, this was resolved. . Basically, by extracting substrings from the values and feeding them individually as date components in the date formula, as I was able to convert the remaining values. . Here&#39;s the snippet: . IF(ISTEXT($E4), DATE( RIGHT($E4,4), MID($E4,4,2), LEFT($E4,2)), $E4) . The IF condition checks if the value is actually a text or a number (since the column contained both) &amp; RIGHT, MID and LEFT are positional string extraction methods. . Here&#39;s the result: . . I replaced commas in the values of the NAME column so that it would be much easier to import the whole dataset into the SQL Server, as a CSV file. . The PRICE column consisted mostly of &quot;Free&quot; (which I replaced with 0), and the remaining values were concatenated with the ₹ currency symbol. Apparently, Excel doesn&#39;t give us any decent inbuilt method to extract numbers from text, and so, I had to take a more comprehensive approach. Here&#39;s the full formula: . VALUE(RIGHT(F5164, LEN(F5164) - MIN(SEARCH({0,1,2,3,4,5,6,7,8,9}, $F5164&amp;&quot;123456789&quot;)) + 1)) . Lets break it down, shall we! . MIN(SEARCH({0,1,2,3,4,5,6,7,8,9}, $F5164&amp;&quot;123456789&quot;) . I use the SEARCH function to find the location of the first digit in the string. Here, the constant array {0,1,2,3,4,5,6,7,8,9} defines all the digits I need to search and $F5164&amp;&quot;123456789&quot; is the target cell, concatenated with string &quot;1234567890&quot; to help provide the SEARCH function with a &quot;fake&quot; position for digits which are not there in the target cell. . Finally, MIN function is used to give the smallest value from the result of the SEARCH function, corrosponding to the location of the first digit in the string. . | RIGHT(F5164, LEN(F5164) - MIN(SEARCH({0,1,2,3,4,5,6,7,8,9}, $F5164&amp;&quot;123456789&quot;)) + 1) . Here we use RIGHT function, which extracts a given length of substring from a string starting from the right side. The first digit location is subtracted from the total length of the main string to get the total number of digits to be extracted. 1 is added to count for the first digit itself. . | VALUE(RIGHT(............................)) . Since Excel will return the substring as a text, which i certainly don&#39;t want, I used the VALUE function to convert them into numbers. . | After the data cleaning was done, I was ready to upload the data to the SQL server. I used the BULK INSERT function to import my CSV into a already made, nice and juicy SQL table. Here&#39;s the full syntax: . bulk insert into WINDOWS_STORE from &quot;/datasets/msft.csv&quot; with(firstrow = 2, type = &quot;csv&quot;) . FIRSTROW parameter is used to tell the function to ignore the header row when importing. . One thing, which I ignored in my early data cleaning shenanigans and caused me a lot of distress while I tried to figure out why my SQL syntax was not working, was quite a few records contained names with special characters, which the VARCHAR datatype doesn&#39;t support. I had to alter my table&#39;s NAME column to have NVARCHAR as its datatype. This solved the problem. . So this is it for now. To conclude, I was able to setup a local containerized development environment on my laptop, cleaned my data and, imported it to my local SQL server instance. . In my next article, I will start with the fun part: Exploring and Visualizing. . &lt;/div&gt; . | .",
            "url": "https://sanchit-agarwal.github.io/2021/07/11/Windows-store-EDA-1.html",
            "relUrl": "/2021/07/11/Windows-store-EDA-1.html",
            "date": " • Jul 11, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Microsoft Word Example Post",
            "content": "When writing a blog post with Microsoft Word – the filename becomes the title. In this case the file name is “2020-01-01-Microsoft-Word-Example-Post.docx”. . There is minimal support for Word documents in fastpages compared to Jupyter notebooks. Some known limitations: . alt text in Word documents are not yet supported by fastpages, and will break links to images. . | You can only specify front matter for Word documents globally. See the README for more details. . | . For greater control over the content produced from Word documents, you will need to convert Word to markdown files manually. You can follow the steps in this blog post, which walk you through how to use pandoc to do the conversion. Note: If you wish to customize your Word generated blog post in markdown, make sure you delete your Word document from the _word directory so your markdown file doesn’t get overwritten! . If your primary method of writing blog posts is Word documents, and you plan on always manually editing Word generated markdown files, you are probably better off using fast_template instead of fastpages. . The material below is a reproduction of this blog post, and serves as an illustrative example. . Maintaining a healthy open source project can entail a huge amount of toil. Popular projects often have orders of magnitude more users and episodic contributors opening issues and PRs than core maintainers capable of handling these issues. . Consider this graphic prepared by the NumFOCUS foundation showing the number of maintainers for three widely used scientific computing projects: . . We can see that across these three projects, there is a very low ratio maintainers to users. Fixing this problem is not an easy task and likely requires innovative solutions to address the economics as well as tools. . Due to its recent momentum and popularity, Kubeflow suffers from a similar fate as illustrated by the growth of new issues opened: . . Source: “TensorFlow World 2019, Automating Your Developer Workflow With ML” . Coincidentally, while building out end to end machine learning examples for Kubeflow, we built two examples using publicly available GitHub data: GitHub Issue Summarization and Code Search. While these tutorials were useful for demonstrating components of Kubeflow, we realized that we could take this a step further and build concrete data products that reduce toil for maintainers. . This is why we started the project kubeflow/code-intelligence, with the goals of increasing project velocity and health using data driven tools. Below are two projects we are currently experimenting with : . Issue Label Bot: This is a bot that automatically labels GitHub issues using Machine Learning. This bot is a GitHub App that was originally built for Kubeflow but is now also used by several large open source projects. The current version of this bot only applies a very limited set of labels, however we are currently A/B testing new models that allow personalized labels. Here is a blog post discussing this project in more detail. . | Issue Triage GitHub Action: to compliment the Issue Label Bot, we created a GitHub Action that automatically adds / removes Issues to the Kubeflow project board tracking issues needing triage. . | Together these projects allow us to reduce the toil of triaging issues. The GitHub Action makes it much easier for the Kubeflow maintainers to track issues needing triage. With the label bot we have taken the first steps in using ML to replace human intervention. We plan on using features extracted by ML to automate more steps in the triage process to further reduce toil. . Building Solutions with GitHub Actions . One of the premises of Kubeflow is that a barrier to building data driven, ML powered solutions is getting models into production and integrated into a solution. In the case of building models to improve OSS project health, that often means integrating with GitHub where the project is hosted. . We are really excited by GitHub’s newly released feature GitHub Actions because we think it will make integrating ML with GitHub much easier. . For simple scripts, like the issue triage script, GitHub actions make it easy to automate executing the script in response to GitHub events without having to build and host a GitHub app. . To automate adding/removing issues needing triage to a Kanban board we wrote a simple python script that interfaces with GitHub’s GraphQL API to modify issues. . As we continue to iterate on ML Models to further reduce toil, GitHub Actions will make it easy to leverage Kubeflow to put our models into production faster. A number of prebuilt GitHub Actions make it easy to create Kubernetes resources in response to GitHub events. For example, we have created GitHub Actions to launch Argo Workflows. This means once we have a Kubernetes job or workflow to perform inference we can easily integrate the model with GitHub and have the full power of Kubeflow and Kubernetes (eg. GPUs). We expect this will allow us to iterate much faster compared to building and maintaining GitHub Apps. . Call To Action . We have a lot more work to do in order to achieve our goal of reducing the amount of toil involved in maintaining OSS projects. If your interested in helping out here’s a couple of issues to get started: . Help us create reports that pull and visualize key performance indicators (KPI). https://github.com/kubeflow/code-intelligence/issues/71 . We have defined our KPI here: issue #19 | . | Combine repo specific and non-repo specific label predictions: https://github.com/kubeflow/code-intelligence/issues/70 . | . In addition to the aforementioned issues we welcome contributions for these other issues in our repo. .",
            "url": "https://sanchit-agarwal.github.io/2020/01/01/Microsoft-Word-Example-Post.html",
            "relUrl": "/2020/01/01/Microsoft-Word-Example-Post.html",
            "date": " • Jan 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Work in progress1. . This means that i haven’t got around to design this page i.e come up with loads of nice things to say about myself, yet. &#8617; . |",
          "url": "https://sanchit-agarwal.github.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://sanchit-agarwal.github.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}